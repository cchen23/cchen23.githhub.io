---
layout: post
title: "AI/ML: Exploration and Exploitation"
date: 2016-12-20
sources: lecture, misc reading
type: summary
---

# We Want to Balance Exploration and Exploitation
During reinforcement learning, we can choose to explore or exploit at each step. If we explore, we try to learn more about the world. If we exploit, we try to choose the action that gives the best payoff right away. For instance, if our goal is to learn the best restaurant for dinner, on a given night we explore by choosing a restaurant we want to learn more about or we can exploit by choosing a restaurant that is our favorite based on previous experiences. If we over-exploit, we might end up making decisions based on insufficient evidence: maybe we go to our favorite restaurant based on past experiences, but never realize that a different restaurant is better than we thought, or that we'd prefer a restaurant that we've never been to. If we over-explore, we could learn a lot about the world but never put that knowledge to good use by "cashing out" on what we've learned from exploration.

# The Multi-Armed Bandit Problem
In a Multi-Armed Bandit scenario, the actor choses an action at each step and receives a reward based on the action. The actor wants to maximize their total reward over all actions.

## Stochastic Bandits
In one version of the MAB, each action corresponds to a distribution centered around an unknown mean. When the actor chooses an action, they receive a reward that is drawn iid from the distribution. The actor could approach this situation in a number of methods, each of which balance exploration and exploitation differently.

### Greedy Method
In this method, the actor estimates each action's value by computing the average reward that the action returned in all previous experiences. In each step, the actor chooses the action with the highest estimated reward. This method favor exploitation over exploration, and the actor could end up choosing suboptimal actions because they did not gather enough evidence from exploration.

### Epsilon-Greedy Method
This method modifies the greedy method to include more exploration. At each step, the actor chooses a random action (with probability epsilon) or chooses the greedy option (with probability 1-epsilon). Although this method initially provides lower expected rewards, over time it performs better than the pure greedy method, since over time the actor learns more about the world through exploration. As time goes on and the actor accumulates more information, it becomes less important to keep exploration. We can use this information to modify the epsilon-greedy method by decreasing the value of epsilon over time. With this improvement, the actor turns increasingly towards exploitation after they've already explored and learned information about various options.

### Greedy Method with Optimistic Initialization
Optimistic initialization can also improve the greedy method by encouraging the actor to spend more time exploring in the beginning. In this method, all actions begin with a high estimated value. Because of this initially high value, unexplored actions have higher estimated values and an actor explores these options using the greedy method.

## Upper Confidence Bound Algorithm
We can balance exploration and exploitation through various improvements to the greedy method, but these improvements randomly select which option to explore. If we have taken an action many times in the past, we learn less by choosing to explore it again because we already have a decent amount of information about the action. This method encourages the actor to explore more uncertain actions by increasing the probability of selecting actions with larger confidence intervals.

## Boltzmann Exploration
In this method, the actor chooses actions probabilistically, preferring actions with higher estimated rewards.

## Adversarial Bandits
In this version of the MAB, the actor's adversary determines the reward of each action at each time ahead of time. The actor does not know anything about these rewards, but the actor's goal is to minimize regret. We define regret as the difference between the maximum possible total reward and the actor's total reward.

### Exponential-weight Algorithm
The actor can approach this problem by choosing each action according to a weight distribution determined by previous rewards. With certain weight update methods the actor can ensure that their expected regret is linearithmic in the number of choices and linear in the maximum possible reward.
