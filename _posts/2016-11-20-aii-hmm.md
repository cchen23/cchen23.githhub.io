---
layout: post
title: "AI: Hidden Markov Models"
date: 2016-11-20
sources: lecture, misc online sources
type: summary
---

## The Structure of Hidden Markov Models
Hidden markov models are markov models that contain two types of nodes: hidden nodes and evidence nodes. We can only observe the values of the evidence nodes.

## Application: Word Tagging
We can use a hidden markov model for word tagging. In this application, we want to tag each word in a sentence with its part of speech. In the hidden markov models, hidden nodes represent tags and evidence nodes represent the words of the sentence.
If we assume that each word depends only on its corresponding tag, and each tag depends only upon the tag that came before it, then we would use the following model (each o represents a word of the sentence, and each x represents a tag):

![first order HMM](http://learning.cis.upenn.edu/cis520_fall2009/uploads/Lectures/hmm.png)

### Computation of Tag Probabilities
A given sentence contains the values of the evidence variables of the HMM. To predict the corresponding hidden variables, or tags, we want to find the most probable sequence of hidden variables given the evidence variables provided by the sentence.
Because of our assumptions (that each word depends only upon its tag and each tag depends only upon the tag that came before it), the probability of a certain tag sequence is given by the product of the probabilities of the tag sequence and of the words of the sentence given those tags.

We want to find the sequence of tags that maximizes this probability, but computing the probability for every possible tag sequence would take a long time. To efficiently find the sequence of tags that maximizes this probability, we can use the Viterbi algorithm. In this algorithm, we start with the first tag in the sequence and find the probability of each tag. Then we turn to the second tag, and for each possible value of the second tag, we find the sequence of (tag1, tag2) with maximum probability. We keep moving forwards until we reach the last tag in the sequence, and at that point we have the full sequence of tags with highest probability. This algorithm works because we assume that each tag depends only on the one before it, so finding the maximum probability sequence of tags up to tag k only requires knowing the maximum probability sequence of tags 1 through k-1 for each possible tag k-1.

#### Computation: Forward-Backward Algorithm for Smoothing
In an HMM in which variables are ordered, we can also predict the value of a specific hidden variable, given all the evidence variables. This computation uses a similar strategy to the tagging algorithm.

First, we perform the forward computation, finding the probability of each value of the hidden variable using all the evidence variables before and up to it. Then we perform the backwards part, computing the probabilities for each value of the hidden variable using all the evidence variables that come after it. If we multiply the forward probability by the backward probability for each possible value of the hidden variable, we have a probability for each possible value of the hidden variable that uses both the evidence variables before/up to the hidden variable and the evidence variables that came after it. We can then predict the value of the hidden variable by choosing the value with the maximum probability.
